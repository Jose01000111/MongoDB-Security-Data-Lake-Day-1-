# ğŸ§  MongoDB SOC Data Lake â€” 5-Phase Lab Project 

#### I want to build a functional SOC data lake in MongoDB on Ubuntu that simulates how security analysts collect, store, correlate, and enrich log data. The goal is to design a professional data model demonstrating SOC workflows, threat intelligence, and data enrichment. This is meant to impress MongoDB leadership with Python-driven ingestion, enrichment, and correlation in a self-contained Ubuntu environment.

#### Tools Iâ€™m Using:

#### ğŸŸ¢ MongoDB Community Edition on Ubuntu

#### ğŸŸ¢ MongoDB Compass

#### ğŸ Python 3 with pymongo

#### ğŸ“Š Sample security data from Wazuh, Sysmon, firewall logs, and threat intelligence feeds

#### ğŸ’» Optional: VS Code or Jupyter Notebook

## Phase 1 â€” Environment Setup & Planning âš™ï¸

## What I Did:
I installed MongoDB Community Edition on Ubuntu using apt commands, set up MongoDB to run as a service, and verified the connection with mongosh. I installed MongoDB Compass for GUI exploration and Python with pymongo for automation. I decided on my log sources: Sysmon logs for system activity, firewall logs for traffic, and Wazuh alerts for IDS data.

## Why:
Setting up MongoDB locally ensures I understand installation, configuration, and connectivity on Ubuntu â€” critical for real-world deployments. Planning my architecture ensures that my data model will handle multiple security sources efficiently.

### ğŸ“¸ Deliverables :

#### ğŸƒTerminal showing MongoDB installed and running (sudo systemctl status mongod)

#### ğŸƒCompass connected to local MongoDB instance

#### ğŸƒPython pymongo setup confirmation

## Phase 2 â€” Collect & Prepare Security Data ğŸ“¥

## What I Did:
I downloaded sample datasets: Sysmon logs, firewall logs, Wazuh IDS alerts, and threat intel feeds. I cleaned and normalized the data (standardizing IP addresses, timestamps, and actions) and converted any CSV or text files into JSON. I organized the files into /data/sysmon/, /data/firewall/, /data/wazuh/, and /data/threat_intel/.

## Why:
Clean, standardized data is essential for proper ingestion and analysis. Normalization ensures queries and correlation pipelines work without errors, just like in a production SOC.

### ğŸ“¸ Deliverables :

#### ğŸƒFolder structure on Ubuntu showing organized JSON files

#### ğŸƒSample JSON file open in VS Code or Jupyter Notebook

#### ğŸƒExample of cleaned and normalized log data

## Phase 3 â€” Design MongoDB Data Model & Ingest Logs ğŸ—„ï¸

## What I Did:
I created a database soc_data with collections: sysmon_logs, firewall_logs, wazuh_alerts, and threat_intel. Using Python and pymongo, I wrote scripts to insert JSON logs into MongoDB. I verified that timestamps, IPs, and event types were correctly interpreted.

## Why:
I focused on data modeling, making sure each collection supports SOC-style queries and enrichment. This is key for demonstrating that I can structure data intelligently for analysis.

### ğŸ“¸ Deliverables :

#### ğŸƒCompass view of soc_data collections with documents

#### ğŸƒPython script inserting JSON data into MongoDB

#### ğŸƒSample document showing correct fields and data types

## Phase 4 â€” Query, Correlate & Enrich ğŸ”

## What I Did:
I wrote queries to find suspicious activity: failed logins, blocked IPs, and repeated alerts. Using $lookup, I correlated firewall IPs with threat intel feeds and enriched alerts with tags like malicious, suspicious, or benign. I also linked Wazuh alerts with Sysmon events for deeper analysis.

## Why:
This phase simulates real SOC analysis, showing that my MongoDB model is ready for analytics and threat detection, not just storage.

### ğŸ“¸ Deliverables :

#### ğŸƒExample MongoDB queries in mongosh

#### ğŸƒCompass showing correlated and enriched data

#### ğŸƒSample enriched documents with added fields

## Phase 5 â€” Visualization & GitHub Documentation ğŸ“Š

## What I Did:
I created visualizations using MongoDB Charts and Python (matplotlib/pandas) showing trends: top blocked IPs, frequent alert types, and correlated threats. I wrote a GitHub README explaining architecture, goals, and instructions to reproduce the data model.

## Why:
This phase demonstrates end-to-end capability: ingestion, enrichment, and actionable insights. Documenting everything professionally shows technical skill and communication â€” exactly what impresses executives.

### ğŸ“¸ Deliverables :

#### ğŸƒMongoDB Charts dashboard of alerts or IPs

#### ğŸƒPython-generated plots showing trends

#### ğŸƒREADME snippet with project structure and workflow


ğŸ Summary

Today I set up my Ubuntu SOC lab, installed all necessary tools, selected log sources, and planned the architecture of my MongoDB SOC Data Lake. The screenshots provide proof of setup and planning, giving me a clear foundation to start log ingestion and normalization in Day 2.
